{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7a629100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xraylib\n",
    "from math import sqrt\n",
    "import torch\n",
    "%matplotlib qt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "62a2d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('panagia1_mm.h5', 'r')\n",
    "dset=f['dataset']\n",
    "dEset=f['energies']\n",
    "spectra_p=np.array(dset)\n",
    "energies=np.array(dEset)\n",
    "f.close()\n",
    "\n",
    "f = h5py.File('christ.h5', 'r')\n",
    "dset=f['dataset']\n",
    "dEset=f['energies']\n",
    "spectra_c=np.array(dset)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95b46a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('spectra.h5', 'r')\n",
    "dset=f['dataset']\n",
    "spectra_h=np.array(dset)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4a7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=100\n",
    "rows_p=41\n",
    "cols_p=65\n",
    "rows_c=31\n",
    "cols_c=46\n",
    "rows_h=41\n",
    "cols_h=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "501f6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_p = spectra_p.astype(np.float32)\n",
    "spectra_c = spectra_c.astype(np.float32)\n",
    "spectra_h = spectra_h.astype(np.float32)\n",
    "spectra_p=np.delete(spectra_p,np.s_[:channels],1)\n",
    "spectra_c=np.delete(spectra_c,np.s_[:channels],1)\n",
    "spectra_h=np.delete(spectra_h,np.s_[:channels],1)\n",
    "\n",
    "energies=np.delete(energies,np.s_[:channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82a0a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f474441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([861, 1948])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b5873a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roi function    \n",
    "def fwhm(ev):\n",
    "    print(sqrt(2.47*ev+4400))\n",
    "    return sqrt(2.47*ev+4400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d2bef77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d34daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_p=torch.tensor(spectra_p)\n",
    "X_c=torch.tensor(spectra_c)\n",
    "X_h=torch.tensor(spectra_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "78c6ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.27226668470861\n",
      "116.27226668470861\n",
      "142.1982770641051\n",
      "142.1982770641051\n",
      "155.7999358151344\n",
      "155.7999358151344\n",
      "170.50756581454092\n",
      "170.50756581454092\n",
      "174.53071362943544\n",
      "174.53071362943544\n",
      "168.49661717672555\n",
      "168.49661717672555\n",
      "[129, 264, 346, 443, 471, 429]\n",
      "[135, 271, 354, 451, 479, 437]\n"
     ]
    }
   ],
   "source": [
    "#dic with energies of elements\n",
    "dic={ 'Ca':3692,'Fe':6405,'Cu':8046,'Hg':9989,'Pb':10551,'Au':9713}\n",
    "key_list=list(dic.keys())\n",
    "min_ch=[]\n",
    "max_ch=[]\n",
    "min_roi_all=[]\n",
    "max_roi_all=[]\n",
    "for element in dic:\n",
    "    min_roi=(dic[element]-fwhm(dic[element])/2)\n",
    "    min_roi_all.append(min_roi)\n",
    "    max_roi=(dic[element]+fwhm(dic[element])/2)\n",
    "    max_roi_all.append(max_roi)\n",
    "    min_ch.append(int((min_roi+960)/20)-channels)\n",
    "    max_ch.append(int((max_roi+960)/20)-channels)\n",
    "print(min_ch)\n",
    "print(max_ch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f535295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861, 1948)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectra_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aad52bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_p_cube=spectra_p.reshape(rows_p,cols_p,spectra_p.shape[1])\n",
    "spectra_c_cube=spectra_c.reshape(rows_c,cols_c,spectra_c.shape[1])\n",
    "spectra_h_cube=spectra_h.reshape(cols_h,rows_h,spectra_h.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb92076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "f1=plt.figure()\n",
    "plt.imshow(np.sum(spectra_h_cube[:,:,min_ch[2]:max_ch[2]],axis=2))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().invert_xaxis()\n",
    "plt.rcParams.update({'font.size': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2c81615",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p=np.zeros([len(spectra_p),6])\n",
    "for i in range(len(spectra_p)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_p[i][j]=np.sum(np.sum(spectra_p[i,min_ch[j]:max_ch[j]]))\n",
    "y_c=np.zeros([len(spectra_c),6])\n",
    "for i in range(len(spectra_c)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_c[i][j]=np.sum(np.sum(spectra_c[i,min_ch[j]:max_ch[j]]))\n",
    "y_h=np.zeros([len(spectra_h),6])\n",
    "for i in range(len(spectra_h)):\n",
    "    for j in range(len(key_list)):\n",
    "        y_h[i][j]=np.sum(np.sum(spectra_h[i,min_ch[j]:max_ch[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b180b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_cube=y_p.reshape(rows_p,cols_p,6)\n",
    "y_c_cube=y_c.reshape(rows_c,cols_c,6)\n",
    "y_h_cube=y_h.reshape(cols_h,rows_h,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4002bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=(100,),activation='relu',solver='adam',verbose=True,max_iter=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "867b21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11636.95099561\n",
      "Iteration 2, loss = 8819.06579037\n",
      "Iteration 3, loss = 6775.11810196\n",
      "Iteration 4, loss = 5106.44676427\n",
      "Iteration 5, loss = 3824.97357580\n",
      "Iteration 6, loss = 2734.36469494\n",
      "Iteration 7, loss = 1854.37505846\n",
      "Iteration 8, loss = 1169.99919444\n",
      "Iteration 9, loss = 708.10143438\n",
      "Iteration 10, loss = 437.41192396\n",
      "Iteration 11, loss = 296.19371852\n",
      "Iteration 12, loss = 224.84236647\n",
      "Iteration 13, loss = 182.28609741\n",
      "Iteration 14, loss = 145.54718723\n",
      "Iteration 15, loss = 113.73966563\n",
      "Iteration 16, loss = 95.53986779\n",
      "Iteration 17, loss = 86.97277144\n",
      "Iteration 18, loss = 79.94574724\n",
      "Iteration 19, loss = 72.68576188\n",
      "Iteration 20, loss = 66.17142442\n",
      "Iteration 21, loss = 61.09206829\n",
      "Iteration 22, loss = 56.86078095\n",
      "Iteration 23, loss = 52.48403984\n",
      "Iteration 24, loss = 48.52653648\n",
      "Iteration 25, loss = 45.09936095\n",
      "Iteration 26, loss = 41.96251747\n",
      "Iteration 27, loss = 38.95139420\n",
      "Iteration 28, loss = 36.22022284\n",
      "Iteration 29, loss = 33.87337068\n",
      "Iteration 30, loss = 31.75429979\n",
      "Iteration 31, loss = 29.83646618\n",
      "Iteration 32, loss = 28.24743044\n",
      "Iteration 33, loss = 26.62234323\n",
      "Iteration 34, loss = 25.30036467\n",
      "Iteration 35, loss = 23.96673199\n",
      "Iteration 36, loss = 22.74515479\n",
      "Iteration 37, loss = 21.84211665\n",
      "Iteration 38, loss = 20.89300424\n",
      "Iteration 39, loss = 20.02152209\n",
      "Iteration 40, loss = 19.35086697\n",
      "Iteration 41, loss = 18.63766560\n",
      "Iteration 42, loss = 17.99648564\n",
      "Iteration 43, loss = 17.39099275\n",
      "Iteration 44, loss = 16.93441154\n",
      "Iteration 45, loss = 16.44131934\n",
      "Iteration 46, loss = 16.20825473\n",
      "Iteration 47, loss = 15.66850849\n",
      "Iteration 48, loss = 15.41095239\n",
      "Iteration 49, loss = 14.92981842\n",
      "Iteration 50, loss = 14.70042277\n",
      "Iteration 51, loss = 14.45315036\n",
      "Iteration 52, loss = 14.12849901\n",
      "Iteration 53, loss = 14.00375500\n",
      "Iteration 54, loss = 13.80047450\n",
      "Iteration 55, loss = 13.84494877\n",
      "Iteration 56, loss = 13.50633603\n",
      "Iteration 57, loss = 13.39194226\n",
      "Iteration 58, loss = 13.11490608\n",
      "Iteration 59, loss = 12.84292147\n",
      "Iteration 60, loss = 12.66903259\n",
      "Iteration 61, loss = 12.43415025\n",
      "Iteration 62, loss = 12.26190567\n",
      "Iteration 63, loss = 12.08944663\n",
      "Iteration 64, loss = 11.88418818\n",
      "Iteration 65, loss = 11.75271244\n",
      "Iteration 66, loss = 11.58190911\n",
      "Iteration 67, loss = 11.43414099\n",
      "Iteration 68, loss = 11.38726989\n",
      "Iteration 69, loss = 11.23389977\n",
      "Iteration 70, loss = 11.03763509\n",
      "Iteration 71, loss = 10.96497950\n",
      "Iteration 72, loss = 10.81432827\n",
      "Iteration 73, loss = 10.64591722\n",
      "Iteration 74, loss = 10.50770477\n",
      "Iteration 75, loss = 10.43619715\n",
      "Iteration 76, loss = 10.25574321\n",
      "Iteration 77, loss = 10.14218252\n",
      "Iteration 78, loss = 10.08728516\n",
      "Iteration 79, loss = 9.87991284\n",
      "Iteration 80, loss = 9.74555686\n",
      "Iteration 81, loss = 9.60994637\n",
      "Iteration 82, loss = 9.47551372\n",
      "Iteration 83, loss = 9.36001520\n",
      "Iteration 84, loss = 9.24793475\n",
      "Iteration 85, loss = 9.14717997\n",
      "Iteration 86, loss = 9.02576869\n",
      "Iteration 87, loss = 8.99007052\n",
      "Iteration 88, loss = 8.84224754\n",
      "Iteration 89, loss = 8.73288638\n",
      "Iteration 90, loss = 8.72815802\n",
      "Iteration 91, loss = 8.57730638\n",
      "Iteration 92, loss = 8.42184174\n",
      "Iteration 93, loss = 8.25307382\n",
      "Iteration 94, loss = 8.18836235\n",
      "Iteration 95, loss = 8.04405080\n",
      "Iteration 96, loss = 7.98019988\n",
      "Iteration 97, loss = 7.88776645\n",
      "Iteration 98, loss = 7.76735286\n",
      "Iteration 99, loss = 7.65956678\n",
      "Iteration 100, loss = 7.63253740\n",
      "Iteration 101, loss = 7.65917584\n",
      "Iteration 102, loss = 7.47063864\n",
      "Iteration 103, loss = 7.38053275\n",
      "Iteration 104, loss = 7.30819349\n",
      "Iteration 105, loss = 7.16045965\n",
      "Iteration 106, loss = 7.08724162\n",
      "Iteration 107, loss = 6.98027293\n",
      "Iteration 108, loss = 6.87904960\n",
      "Iteration 109, loss = 6.75162102\n",
      "Iteration 110, loss = 6.69286652\n",
      "Iteration 111, loss = 6.63685696\n",
      "Iteration 112, loss = 6.49584852\n",
      "Iteration 113, loss = 6.49408322\n",
      "Iteration 114, loss = 6.34780504\n",
      "Iteration 115, loss = 6.26841559\n",
      "Iteration 116, loss = 6.19423848\n",
      "Iteration 117, loss = 6.19260386\n",
      "Iteration 118, loss = 6.05315773\n",
      "Iteration 119, loss = 5.99994714\n",
      "Iteration 120, loss = 5.89373461\n",
      "Iteration 121, loss = 5.77979759\n",
      "Iteration 122, loss = 5.83208446\n",
      "Iteration 123, loss = 5.69691378\n",
      "Iteration 124, loss = 5.60396564\n",
      "Iteration 125, loss = 5.54425037\n",
      "Iteration 126, loss = 5.48247628\n",
      "Iteration 127, loss = 5.41055872\n",
      "Iteration 128, loss = 5.31870623\n",
      "Iteration 129, loss = 5.29734410\n",
      "Iteration 130, loss = 5.24838560\n",
      "Iteration 131, loss = 5.09999565\n",
      "Iteration 132, loss = 4.99399606\n",
      "Iteration 133, loss = 4.96319593\n",
      "Iteration 134, loss = 4.89059470\n",
      "Iteration 135, loss = 4.86848069\n",
      "Iteration 136, loss = 4.78201181\n",
      "Iteration 137, loss = 4.68621649\n",
      "Iteration 138, loss = 4.64198142\n",
      "Iteration 139, loss = 4.58252876\n",
      "Iteration 140, loss = 4.57961232\n",
      "Iteration 141, loss = 4.47743565\n",
      "Iteration 142, loss = 4.47579580\n",
      "Iteration 143, loss = 4.37750348\n",
      "Iteration 144, loss = 4.29909829\n",
      "Iteration 145, loss = 4.25189692\n",
      "Iteration 146, loss = 4.22363768\n",
      "Iteration 147, loss = 4.23400333\n",
      "Iteration 148, loss = 4.10282250\n",
      "Iteration 149, loss = 4.02021987\n",
      "Iteration 150, loss = 3.98437009\n",
      "Iteration 151, loss = 3.97521090\n",
      "Iteration 152, loss = 3.94799306\n",
      "Iteration 153, loss = 3.87562579\n",
      "Iteration 154, loss = 3.83171057\n",
      "Iteration 155, loss = 3.80661333\n",
      "Iteration 156, loss = 3.73730261\n",
      "Iteration 157, loss = 3.75458776\n",
      "Iteration 158, loss = 3.67168489\n",
      "Iteration 159, loss = 3.62231315\n",
      "Iteration 160, loss = 3.51545165\n",
      "Iteration 161, loss = 3.48563096\n",
      "Iteration 162, loss = 3.40094600\n",
      "Iteration 163, loss = 3.37422547\n",
      "Iteration 164, loss = 3.33063437\n",
      "Iteration 165, loss = 3.29585764\n",
      "Iteration 166, loss = 3.24560929\n",
      "Iteration 167, loss = 3.24130675\n",
      "Iteration 168, loss = 3.29055625\n",
      "Iteration 169, loss = 3.17301614\n",
      "Iteration 170, loss = 3.10659764\n",
      "Iteration 171, loss = 3.05953621\n",
      "Iteration 172, loss = 2.99902548\n",
      "Iteration 173, loss = 3.01277766\n",
      "Iteration 174, loss = 2.95332829\n",
      "Iteration 175, loss = 2.89676546\n",
      "Iteration 176, loss = 2.86241753\n",
      "Iteration 177, loss = 2.79961867\n",
      "Iteration 178, loss = 2.77296076\n",
      "Iteration 179, loss = 2.77271291\n",
      "Iteration 180, loss = 2.73491907\n",
      "Iteration 181, loss = 2.71450599\n",
      "Iteration 182, loss = 2.65505626\n",
      "Iteration 183, loss = 2.61505136\n",
      "Iteration 184, loss = 2.57398954\n",
      "Iteration 185, loss = 2.52089287\n",
      "Iteration 186, loss = 2.49478738\n",
      "Iteration 187, loss = 2.49694710\n",
      "Iteration 188, loss = 2.48867696\n",
      "Iteration 189, loss = 2.47008120\n",
      "Iteration 190, loss = 2.46239661\n",
      "Iteration 191, loss = 2.54136015\n",
      "Iteration 192, loss = 2.43559326\n",
      "Iteration 193, loss = 2.36328820\n",
      "Iteration 194, loss = 2.25002167\n",
      "Iteration 195, loss = 2.23521885\n",
      "Iteration 196, loss = 2.21250677\n",
      "Iteration 197, loss = 2.18089593\n",
      "Iteration 198, loss = 2.13232323\n",
      "Iteration 199, loss = 2.13889992\n",
      "Iteration 200, loss = 2.12024379\n",
      "Iteration 201, loss = 2.08004577\n",
      "Iteration 202, loss = 2.06334157\n",
      "Iteration 203, loss = 2.01523364\n",
      "Iteration 204, loss = 1.98433617\n",
      "Iteration 205, loss = 1.97896223\n",
      "Iteration 206, loss = 1.94846271\n",
      "Iteration 207, loss = 1.94689693\n",
      "Iteration 208, loss = 1.91046420\n",
      "Iteration 209, loss = 1.87919911\n",
      "Iteration 210, loss = 1.85835336\n",
      "Iteration 211, loss = 1.82430493\n",
      "Iteration 212, loss = 1.79904767\n",
      "Iteration 213, loss = 1.78497949\n",
      "Iteration 214, loss = 1.77211970\n",
      "Iteration 215, loss = 1.75046379\n",
      "Iteration 216, loss = 1.72889418\n",
      "Iteration 217, loss = 1.72253946\n",
      "Iteration 218, loss = 1.66798099\n",
      "Iteration 219, loss = 1.66010944\n",
      "Iteration 220, loss = 1.61265076\n",
      "Iteration 221, loss = 1.59850043\n",
      "Iteration 222, loss = 1.58694157\n",
      "Iteration 223, loss = 1.56015420\n",
      "Iteration 224, loss = 1.58038944\n",
      "Iteration 225, loss = 1.53960077\n",
      "Iteration 226, loss = 1.52837380\n",
      "Iteration 227, loss = 1.48359872\n",
      "Iteration 228, loss = 1.47695484\n",
      "Iteration 229, loss = 1.49916275\n",
      "Iteration 230, loss = 1.47206800\n",
      "Iteration 231, loss = 1.45460701\n",
      "Iteration 232, loss = 1.46575711\n",
      "Iteration 233, loss = 1.44377655\n",
      "Iteration 234, loss = 1.38153036\n",
      "Iteration 235, loss = 1.38116027\n",
      "Iteration 236, loss = 1.35358712\n",
      "Iteration 237, loss = 1.33918918\n",
      "Iteration 238, loss = 1.30809039\n",
      "Iteration 239, loss = 1.29501106\n",
      "Iteration 240, loss = 1.27800520\n",
      "Iteration 241, loss = 1.25952496\n",
      "Iteration 242, loss = 1.24287197\n",
      "Iteration 243, loss = 1.24216799\n",
      "Iteration 244, loss = 1.20031898\n",
      "Iteration 245, loss = 1.19830066\n",
      "Iteration 246, loss = 1.19815851\n",
      "Iteration 247, loss = 1.17969339\n",
      "Iteration 248, loss = 1.15199883\n",
      "Iteration 249, loss = 1.15245293\n",
      "Iteration 250, loss = 1.14937416\n",
      "Iteration 251, loss = 1.12040774\n",
      "Iteration 252, loss = 1.08970300\n",
      "Iteration 253, loss = 1.09852603\n",
      "Iteration 254, loss = 1.07718458\n",
      "Iteration 255, loss = 1.06584630\n",
      "Iteration 256, loss = 1.06275523\n",
      "Iteration 257, loss = 1.06944136\n",
      "Iteration 258, loss = 1.06754413\n",
      "Iteration 259, loss = 1.00672624\n",
      "Iteration 260, loss = 1.00948495\n",
      "Iteration 261, loss = 0.99510629\n",
      "Iteration 262, loss = 0.98469456\n",
      "Iteration 263, loss = 0.96148603\n",
      "Iteration 264, loss = 0.94542278\n",
      "Iteration 265, loss = 0.94800536\n",
      "Iteration 266, loss = 0.93718737\n",
      "Iteration 267, loss = 0.91358407\n",
      "Iteration 268, loss = 0.91616502\n",
      "Iteration 269, loss = 0.89533433\n",
      "Iteration 270, loss = 0.87806866\n",
      "Iteration 271, loss = 0.89606389\n",
      "Iteration 272, loss = 0.87511569\n",
      "Iteration 273, loss = 0.87111479\n",
      "Iteration 274, loss = 0.88792641\n",
      "Iteration 275, loss = 0.90536266\n",
      "Iteration 276, loss = 0.86325715\n",
      "Iteration 277, loss = 0.85215369\n",
      "Iteration 278, loss = 0.83621777\n",
      "Iteration 279, loss = 0.80559446\n",
      "Iteration 280, loss = 0.79305007\n",
      "Iteration 281, loss = 0.78586424\n",
      "Iteration 282, loss = 0.78203585\n",
      "Iteration 283, loss = 0.80324217\n",
      "Iteration 284, loss = 0.81659231\n",
      "Iteration 285, loss = 0.82452674\n",
      "Iteration 286, loss = 0.80278188\n",
      "Iteration 287, loss = 0.75982704\n",
      "Iteration 288, loss = 0.76022994\n",
      "Iteration 289, loss = 0.72341505\n",
      "Iteration 290, loss = 0.71873849\n",
      "Iteration 291, loss = 0.69107776\n",
      "Iteration 292, loss = 0.68658778\n",
      "Iteration 293, loss = 0.67540921\n",
      "Iteration 294, loss = 0.66630551\n",
      "Iteration 295, loss = 0.66477598\n",
      "Iteration 296, loss = 0.66120714\n",
      "Iteration 297, loss = 0.65786140\n",
      "Iteration 298, loss = 0.65184703\n",
      "Iteration 299, loss = 0.64457230\n",
      "Iteration 300, loss = 0.64298181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_h, y_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7aa9c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 29065.32217228\n",
      "Iteration 2, loss = 13611.43165639\n",
      "Iteration 3, loss = 6307.96675691\n",
      "Iteration 4, loss = 2967.18782684\n",
      "Iteration 5, loss = 1182.17284455\n",
      "Iteration 6, loss = 432.57502525\n",
      "Iteration 7, loss = 257.16810740\n",
      "Iteration 8, loss = 190.75018614\n",
      "Iteration 9, loss = 140.40339158\n",
      "Iteration 10, loss = 109.43844617\n",
      "Iteration 11, loss = 90.71681345\n",
      "Iteration 12, loss = 77.93291224\n",
      "Iteration 13, loss = 69.64514239\n",
      "Iteration 14, loss = 62.54706842\n",
      "Iteration 15, loss = 56.89507483\n",
      "Iteration 16, loss = 51.64912719\n",
      "Iteration 17, loss = 47.54488944\n",
      "Iteration 18, loss = 44.23811090\n",
      "Iteration 19, loss = 41.85407203\n",
      "Iteration 20, loss = 39.67580951\n",
      "Iteration 21, loss = 38.10533640\n",
      "Iteration 22, loss = 36.51889163\n",
      "Iteration 23, loss = 35.38178581\n",
      "Iteration 24, loss = 34.35117407\n",
      "Iteration 25, loss = 33.39922241\n",
      "Iteration 26, loss = 32.85181279\n",
      "Iteration 27, loss = 32.07761387\n",
      "Iteration 28, loss = 31.61805354\n",
      "Iteration 29, loss = 30.94990335\n",
      "Iteration 30, loss = 30.46206949\n",
      "Iteration 31, loss = 30.02033232\n",
      "Iteration 32, loss = 29.59910836\n",
      "Iteration 33, loss = 29.10166323\n",
      "Iteration 34, loss = 28.92677215\n",
      "Iteration 35, loss = 28.27553035\n",
      "Iteration 36, loss = 27.77701996\n",
      "Iteration 37, loss = 27.50248604\n",
      "Iteration 38, loss = 27.42637732\n",
      "Iteration 39, loss = 26.90845695\n",
      "Iteration 40, loss = 26.26467035\n",
      "Iteration 41, loss = 25.98528728\n",
      "Iteration 42, loss = 25.66645806\n",
      "Iteration 43, loss = 25.28703728\n",
      "Iteration 44, loss = 25.23121804\n",
      "Iteration 45, loss = 25.07461534\n",
      "Iteration 46, loss = 24.27932592\n",
      "Iteration 47, loss = 24.06191334\n",
      "Iteration 48, loss = 23.63886525\n",
      "Iteration 49, loss = 23.59518901\n",
      "Iteration 50, loss = 22.80263698\n",
      "Iteration 51, loss = 22.47184641\n",
      "Iteration 52, loss = 22.24462710\n",
      "Iteration 53, loss = 22.06502217\n",
      "Iteration 54, loss = 21.60444571\n",
      "Iteration 55, loss = 21.38661599\n",
      "Iteration 56, loss = 20.93122349\n",
      "Iteration 57, loss = 20.66902955\n",
      "Iteration 58, loss = 20.46762344\n",
      "Iteration 59, loss = 19.87987419\n",
      "Iteration 60, loss = 20.13988465\n",
      "Iteration 61, loss = 19.52942090\n",
      "Iteration 62, loss = 19.44072048\n",
      "Iteration 63, loss = 19.21695442\n",
      "Iteration 64, loss = 19.00483678\n",
      "Iteration 65, loss = 18.50021952\n",
      "Iteration 66, loss = 18.32027228\n",
      "Iteration 67, loss = 18.35075380\n",
      "Iteration 68, loss = 17.93942978\n",
      "Iteration 69, loss = 17.66020123\n",
      "Iteration 70, loss = 17.30684048\n",
      "Iteration 71, loss = 16.98548092\n",
      "Iteration 72, loss = 16.71725547\n",
      "Iteration 73, loss = 16.73389864\n",
      "Iteration 74, loss = 16.59562564\n",
      "Iteration 75, loss = 15.91803435\n",
      "Iteration 76, loss = 15.91362657\n",
      "Iteration 77, loss = 15.68088307\n",
      "Iteration 78, loss = 15.21742403\n",
      "Iteration 79, loss = 14.96907212\n",
      "Iteration 80, loss = 14.86529357\n",
      "Iteration 81, loss = 14.73443215\n",
      "Iteration 82, loss = 14.87603025\n",
      "Iteration 83, loss = 14.42234975\n",
      "Iteration 84, loss = 14.28619966\n",
      "Iteration 85, loss = 13.75421654\n",
      "Iteration 86, loss = 13.62722265\n",
      "Iteration 87, loss = 13.33392824\n",
      "Iteration 88, loss = 13.15737342\n",
      "Iteration 89, loss = 13.05881081\n",
      "Iteration 90, loss = 12.94555911\n",
      "Iteration 91, loss = 12.56395224\n",
      "Iteration 92, loss = 12.46511373\n",
      "Iteration 93, loss = 12.22244958\n",
      "Iteration 94, loss = 12.16112098\n",
      "Iteration 95, loss = 12.22210232\n",
      "Iteration 96, loss = 11.84201276\n",
      "Iteration 97, loss = 11.59573626\n",
      "Iteration 98, loss = 11.32280889\n",
      "Iteration 99, loss = 11.20548432\n",
      "Iteration 100, loss = 11.03969018\n",
      "Iteration 101, loss = 11.05435455\n",
      "Iteration 102, loss = 11.04148395\n",
      "Iteration 103, loss = 10.46206752\n",
      "Iteration 104, loss = 10.87094921\n",
      "Iteration 105, loss = 10.42068271\n",
      "Iteration 106, loss = 10.15232953\n",
      "Iteration 107, loss = 9.98932421\n",
      "Iteration 108, loss = 9.77994155\n",
      "Iteration 109, loss = 9.74022670\n",
      "Iteration 110, loss = 9.59446541\n",
      "Iteration 111, loss = 9.23290762\n",
      "Iteration 112, loss = 9.21575491\n",
      "Iteration 113, loss = 9.15015713\n",
      "Iteration 114, loss = 8.94287747\n",
      "Iteration 115, loss = 8.71463706\n",
      "Iteration 116, loss = 9.04616262\n",
      "Iteration 117, loss = 8.70405204\n",
      "Iteration 118, loss = 8.54160968\n",
      "Iteration 119, loss = 8.57821053\n",
      "Iteration 120, loss = 8.33490415\n",
      "Iteration 121, loss = 8.21989361\n",
      "Iteration 122, loss = 8.06157915\n",
      "Iteration 123, loss = 7.83006258\n",
      "Iteration 124, loss = 7.75270639\n",
      "Iteration 125, loss = 7.72251585\n",
      "Iteration 126, loss = 7.42573076\n",
      "Iteration 127, loss = 7.41682639\n",
      "Iteration 128, loss = 7.65319757\n",
      "Iteration 129, loss = 7.49377440\n",
      "Iteration 130, loss = 7.16541901\n",
      "Iteration 131, loss = 7.00734785\n",
      "Iteration 132, loss = 6.86005004\n",
      "Iteration 133, loss = 6.66764279\n",
      "Iteration 134, loss = 6.65720233\n",
      "Iteration 135, loss = 6.68243194\n",
      "Iteration 136, loss = 6.35862382\n",
      "Iteration 137, loss = 6.30889813\n",
      "Iteration 138, loss = 6.27955091\n",
      "Iteration 139, loss = 6.21689128\n",
      "Iteration 140, loss = 6.27457023\n",
      "Iteration 141, loss = 6.63561613\n",
      "Iteration 142, loss = 6.17772974\n",
      "Iteration 143, loss = 6.00373113\n",
      "Iteration 144, loss = 5.96689191\n",
      "Iteration 145, loss = 5.73905031\n",
      "Iteration 146, loss = 5.58740247\n",
      "Iteration 147, loss = 5.43446582\n",
      "Iteration 148, loss = 5.79889078\n",
      "Iteration 149, loss = 5.74961350\n",
      "Iteration 150, loss = 5.36248341\n",
      "Iteration 151, loss = 5.38170408\n",
      "Iteration 152, loss = 5.06961861\n",
      "Iteration 153, loss = 4.98393065\n",
      "Iteration 154, loss = 4.96122247\n",
      "Iteration 155, loss = 5.00489597\n",
      "Iteration 156, loss = 5.00926500\n",
      "Iteration 157, loss = 4.86388545\n",
      "Iteration 158, loss = 4.93844717\n",
      "Iteration 159, loss = 4.66888726\n",
      "Iteration 160, loss = 4.60159116\n",
      "Iteration 161, loss = 4.63902089\n",
      "Iteration 162, loss = 4.40995111\n",
      "Iteration 163, loss = 4.40242563\n",
      "Iteration 164, loss = 4.27326081\n",
      "Iteration 165, loss = 4.14359201\n",
      "Iteration 166, loss = 4.21691294\n",
      "Iteration 167, loss = 4.19923276\n",
      "Iteration 168, loss = 4.10292310\n",
      "Iteration 169, loss = 4.01546446\n",
      "Iteration 170, loss = 3.94126448\n",
      "Iteration 171, loss = 3.90200224\n",
      "Iteration 172, loss = 3.77948557\n",
      "Iteration 173, loss = 3.90861268\n",
      "Iteration 174, loss = 3.96119980\n",
      "Iteration 175, loss = 3.79207623\n",
      "Iteration 176, loss = 3.76099290\n",
      "Iteration 177, loss = 3.71597950\n",
      "Iteration 178, loss = 3.55792557\n",
      "Iteration 179, loss = 3.41092557\n",
      "Iteration 180, loss = 3.39069508\n",
      "Iteration 181, loss = 3.46972421\n",
      "Iteration 182, loss = 3.50976976\n",
      "Iteration 183, loss = 3.37318423\n",
      "Iteration 184, loss = 3.41252795\n",
      "Iteration 185, loss = 3.26439781\n",
      "Iteration 186, loss = 3.12830948\n",
      "Iteration 187, loss = 3.11051619\n",
      "Iteration 188, loss = 3.08577825\n",
      "Iteration 189, loss = 3.08934450\n",
      "Iteration 190, loss = 3.22794320\n",
      "Iteration 191, loss = 3.11876806\n",
      "Iteration 192, loss = 2.99323036\n",
      "Iteration 193, loss = 2.93802499\n",
      "Iteration 194, loss = 2.88798766\n",
      "Iteration 195, loss = 2.86186414\n",
      "Iteration 196, loss = 2.89179283\n",
      "Iteration 197, loss = 2.87319541\n",
      "Iteration 198, loss = 2.85731107\n",
      "Iteration 199, loss = 2.73301221\n",
      "Iteration 200, loss = 2.95107642\n",
      "Iteration 201, loss = 3.01086616\n",
      "Iteration 202, loss = 2.87323594\n",
      "Iteration 203, loss = 2.93288559\n",
      "Iteration 204, loss = 2.87671530\n",
      "Iteration 205, loss = 2.64216169\n",
      "Iteration 206, loss = 2.68747429\n",
      "Iteration 207, loss = 2.80951828\n",
      "Iteration 208, loss = 2.79766603\n",
      "Iteration 209, loss = 2.58364276\n",
      "Iteration 210, loss = 2.41917667\n",
      "Iteration 211, loss = 2.42975640\n",
      "Iteration 212, loss = 2.32068765\n",
      "Iteration 213, loss = 2.27765236\n",
      "Iteration 214, loss = 2.28376756\n",
      "Iteration 215, loss = 2.20829073\n",
      "Iteration 216, loss = 2.20490704\n",
      "Iteration 217, loss = 2.30168811\n",
      "Iteration 218, loss = 2.49801654\n",
      "Iteration 219, loss = 2.20590259\n",
      "Iteration 220, loss = 2.19665007\n",
      "Iteration 221, loss = 2.22072347\n",
      "Iteration 222, loss = 2.28275267\n",
      "Iteration 223, loss = 2.25646705\n",
      "Iteration 224, loss = 2.10808890\n",
      "Iteration 225, loss = 1.99564725\n",
      "Iteration 226, loss = 1.92969864\n",
      "Iteration 227, loss = 2.01553300\n",
      "Iteration 228, loss = 1.93475399\n",
      "Iteration 229, loss = 1.92732451\n",
      "Iteration 230, loss = 1.89726983\n",
      "Iteration 231, loss = 1.81082279\n",
      "Iteration 232, loss = 1.95140364\n",
      "Iteration 233, loss = 1.80385786\n",
      "Iteration 234, loss = 1.75388157\n",
      "Iteration 235, loss = 1.73549322\n",
      "Iteration 236, loss = 1.82985636\n",
      "Iteration 237, loss = 1.91581179\n",
      "Iteration 238, loss = 1.81701117\n",
      "Iteration 239, loss = 1.65724539\n",
      "Iteration 240, loss = 1.74850395\n",
      "Iteration 241, loss = 1.87580388\n",
      "Iteration 242, loss = 1.59475587\n",
      "Iteration 243, loss = 1.57312848\n",
      "Iteration 244, loss = 1.60966329\n",
      "Iteration 245, loss = 1.72522765\n",
      "Iteration 246, loss = 1.65113477\n",
      "Iteration 247, loss = 1.63320739\n",
      "Iteration 248, loss = 1.80158620\n",
      "Iteration 249, loss = 1.97374908\n",
      "Iteration 250, loss = 1.80260729\n",
      "Iteration 251, loss = 1.52801543\n",
      "Iteration 252, loss = 1.60893139\n",
      "Iteration 253, loss = 1.42187953\n",
      "Iteration 254, loss = 1.37464083\n",
      "Iteration 255, loss = 1.45919747\n",
      "Iteration 256, loss = 1.46823355\n",
      "Iteration 257, loss = 1.44616512\n",
      "Iteration 258, loss = 1.34386678\n",
      "Iteration 259, loss = 1.39270730\n",
      "Iteration 260, loss = 1.46362961\n",
      "Iteration 261, loss = 1.40557073\n",
      "Iteration 262, loss = 1.32905669\n",
      "Iteration 263, loss = 1.25995656\n",
      "Iteration 264, loss = 1.35756562\n",
      "Iteration 265, loss = 1.48993701\n",
      "Iteration 266, loss = 1.31163024\n",
      "Iteration 267, loss = 1.29972133\n",
      "Iteration 268, loss = 1.24237943\n",
      "Iteration 269, loss = 1.23917824\n",
      "Iteration 270, loss = 1.23895331\n",
      "Iteration 271, loss = 1.31151344\n",
      "Iteration 272, loss = 1.30247763\n",
      "Iteration 273, loss = 1.23936283\n",
      "Iteration 274, loss = 1.24633557\n",
      "Iteration 275, loss = 1.18500427\n",
      "Iteration 276, loss = 1.18849133\n",
      "Iteration 277, loss = 1.10704061\n",
      "Iteration 278, loss = 1.09507999\n",
      "Iteration 279, loss = 1.21278529\n",
      "Iteration 280, loss = 1.07511101\n",
      "Iteration 281, loss = 1.08897188\n",
      "Iteration 282, loss = 1.21249630\n",
      "Iteration 283, loss = 1.07520020\n",
      "Iteration 284, loss = 1.34960353\n",
      "Iteration 285, loss = 1.28483974\n",
      "Iteration 286, loss = 1.16878554\n",
      "Iteration 287, loss = 1.11887518\n",
      "Iteration 288, loss = 1.06859793\n",
      "Iteration 289, loss = 1.01957645\n",
      "Iteration 290, loss = 1.22089326\n",
      "Iteration 291, loss = 1.05510149\n",
      "Iteration 292, loss = 0.94863399\n",
      "Iteration 293, loss = 0.97908516\n",
      "Iteration 294, loss = 0.93262512\n",
      "Iteration 295, loss = 1.01351466\n",
      "Iteration 296, loss = 1.11052103\n",
      "Iteration 297, loss = 1.07152049\n",
      "Iteration 298, loss = 1.04173047\n",
      "Iteration 299, loss = 0.91368493\n",
      "Iteration 300, loss = 0.97569649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_p, y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "20b7582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=regr.predict(X_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a166b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1426, 1948])\n",
      "torch.Size([2665, 1948])\n"
     ]
    }
   ],
   "source": [
    "print(X_c.shape)\n",
    "print(X_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3232754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.74102696  15.29903956   6.84828006 677.52535638  45.36729455\n",
      "   5.92421009] [ 52.86000061  27.24999809   7.36999989  11.98999977 158.17999268\n",
      "   4.94999981]\n"
     ]
    }
   ],
   "source": [
    "print(a[1],y_c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a2fca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.reshape(cols_h,rows_h,6)\n",
    "f2=plt.figure()\n",
    "plt.imshow(b[:,:,2])\n",
    "#plt.title()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.gca().invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cf1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
